{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evening Hade\n",
    "### Lab 3, Part 2\n",
    "#### For GIS 5571\n",
    "\n",
    "In this part of the lab, I will make an ETL for real-time data, clean & project that data, and interpolate it in different ways.\n",
    "\n",
    "I always start with imports, setting up my project, map, and spatial reference, and defining my output path where everything will get saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory is already assigned to:  C:/ArcGIS/Projects/Lab3-2\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import arcpy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "#Set up arcpro project\n",
    "project = arcpy.mp.ArcGISProject(\"CURRENT\")\n",
    "map = project.listMaps()[0]\n",
    "ref = arcpy.SpatialReference(4326)  #WGS 1983\n",
    "\n",
    "#Path where everything will be saved\n",
    "outputPath = \"C:/ArcGIS/Projects/Lab3-2\"\n",
    "\n",
    "if not os.path.exists(outputPath): #Test that outputPath exists\n",
    "    os.makedirs(outputPath)\n",
    "    print(\"Created output directory: \", outputPath)\n",
    "else:\n",
    "    print(\"Output directory is already assigned to: \", outputPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After initial setup, I need to get my data. I made a list of all the stations currently in the NDAWN system. For the URL, I removed the temporal information \"year\", \"start date\", and \"end date\". This means that this URL will default to whatever day it is when you run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to: C:/ArcGIS/Projects/Lab3-2\\NDAWN_data.csv\n"
     ]
    }
   ],
   "source": [
    "#URL and get request\n",
    "urlBase = \"https://ndawn.ndsu.nodak.edu/table.csv?station=\"\n",
    "\n",
    "stations = ( #List of stations (all of them)\n",
    "    \"78,111,98,162,174,142,164,138,161,9,160,224,159,10,229,118,56,165,11,12,58,\"\n",
    "    \"13,84,218,55,179,7,186,87,14,15,96,191,16,210,201,137,124,143,17,85,226,140,\"\n",
    "    \"134,18,136,219,65,104,99,192,19,227,129,20,101,166,178,81,21,97,22,75,184,2,\"\n",
    "    \"211,172,139,158,23,157,220,62,86,24,89,126,223,167,93,183,90,25,205,83,107,\"\n",
    "    \"156,77,26,155,70,127,144,27,173,132,28,195,185,29,30,154,31,187,102,32,119,\"\n",
    "    \"4,217,80,33,59,153,105,82,225,34,198,72,135,35,76,120,209,141,109,36,207,79,\"\n",
    "    \"193,71,212,37,38,189,39,130,73,188,40,41,54,228,69,194,145,214,113,128,42,43,\"\n",
    "    \"103,171,116,196,88,114,3,163,200,216,64,115,168,67,175,146,170,197,44,206,133,\"\n",
    "    \"106,100,121,45,46,61,66,181,74,213,60,199,125,176,177,8,180,204,47,221,122,\"\n",
    "    \"108,5,152,48,151,147,68,169,49,50,91,182,117,63,150,51,6,222,52,92,112,131,\"\n",
    "    \"123,95,53,203,190,208,57,149,148,202,215,110\"\n",
    ")\n",
    "\n",
    "#Replace commas with '&station=' for use with requests\n",
    "andStation = stations.replace(\",\", \"&station=\")\n",
    "\n",
    "#Construct full URL\n",
    "url = f\"{urlBase}{andStation}\"\n",
    "\n",
    "#Additional parameters for 30-day temp normals\n",
    "params = {\n",
    "    \"variable\": \"ddavt\",\n",
    "    \"ttype\": \"daily\",\n",
    "    \"quick_pick\": \"30_d\"\n",
    "}\n",
    "\n",
    "#Extract data\n",
    "response = requests.get(url, params=params)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.text\n",
    "    rawPath = os.path.join(outputPath, \"NDAWN_data.csv\")\n",
    "    with open(rawPath, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(data)\n",
    "    print(f\"Data saved to: {rawPath}\")\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}, {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I quickly learned that this .csv would need to be manipulated before it could be used. First, I need to remove the random rows with some supplementary information about the table. Then, I need to remove the row with the units. I didn't remove all the string rows because that would leave me without field names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV successfully read into DataFrame.\n"
     ]
    }
   ],
   "source": [
    "#Clean the data using pandas\n",
    "\n",
    "csvPath = os.path.join(outputPath, \"clean_data.csv\")\n",
    "\n",
    "stringsRemoved = [\n",
    "    \"Data from North Dakota Agricultural Weather Network https://ndawn.ndsu.nodak.edu\",\n",
    "    \"Daily Observation Table for\",\n",
    "    \"Flag Definition Line: M - Missing; E - Estimated; N/A - Not Available\"\n",
    "]\n",
    "\n",
    "#Clean the CSV file\n",
    "with open(rawPath, 'r', encoding='utf-8') as infile, open(csvPath, 'w', encoding='utf-8') as outfile:\n",
    "    for line in infile:\n",
    "        # Remove any leading/trailing whitespace characters\n",
    "        line = line.strip()\n",
    "        # Check if the line starts with any of the specified strings\n",
    "        if not any(line.startswith(s) for s in stringsRemoved) and line:\n",
    "            outfile.write(line + '\\n')\n",
    "\n",
    "#Attempt to read the cleaned CSV into a DataFrame using pandas\n",
    "try:\n",
    "    df = pd.read_csv(csvPath, on_bad_lines='skip')\n",
    "    print(\"CSV successfully read into DataFrame.\")\n",
    "except pd.errors.ParserError as e:\n",
    "    print(\"Error reading CSV into DataFrame:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted units row. Converted Longitude, Latitude, and Elevation fields to numeric values.\n",
      "Updated CSV saved at: C:/ArcGIS/Projects/Lab3-2\\clean_data.csv\n"
     ]
    }
   ],
   "source": [
    "#More data cleaning\n",
    "#Someone at NDAWN needs a performance review is2g\n",
    "\n",
    "# Step 2: Remove rows where the \"Longitude\" field has the value \"deg\"\n",
    "df = df[df['Longitude'] != 'deg']\n",
    "df['Longitude'] = pd.to_numeric(df['Longitude'], errors='coerce')\n",
    "df['Latitude'] = pd.to_numeric(df['Latitude'], errors='coerce')\n",
    "df['Elevation'] = pd.to_numeric(df['Elevation'], errors='coerce')\n",
    "\n",
    "print(\"Deleted units row. Converted Longitude, Latitude, and Elevation fields to numeric values.\")\n",
    "\n",
    "# Step 4: Save the cleaned DataFrame back to the CSV\n",
    "df.to_csv(csvPath, index=False, encoding='utf-8')\n",
    "print(f\"Updated CSV saved at: {csvPath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data in the .csv can easily be used by Esri with no random string values, I can add the table to the project and turn it into a feature layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table added to the project.\n"
     ]
    }
   ],
   "source": [
    "if not arcpy.Exists(csvPath):\n",
    "    print(f\"Error: The file {csvPath} was not found.\")\n",
    "else:\n",
    "    arcpy.TableToTable_conversion(in_rows=csvPath, out_path=project.defaultGeodatabase, out_name=\"clean_data\")\n",
    "    print(f\"Table added to the project.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted table to point feature layer.\n"
     ]
    }
   ],
   "source": [
    "#Turn the table into a feature layer\n",
    "\n",
    "arcpy.management.XYTableToPoint(\n",
    "    in_table=csvPath,\n",
    "    out_feature_class=r\"C:\\ArcGIS\\Projects\\Lab3-2\\NDAWN_Points\",\n",
    "    x_field=\"Longitude\",\n",
    "    y_field=\"Latitude\"\n",
    ")\n",
    "\n",
    "print(\"Converted table to point feature layer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this point feature layer, I can run spatial interpolations.\n",
    "\n",
    "I did Kernel with Barriers first because I like how its extent \"follows\" the points without using a polygon/polyline barrier layer. I chose it because it predicts values and errors while also having one value per location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created kernel interpolation with barriers and saved to C:/ArcGIS/Projects/Lab3-2.\n"
     ]
    }
   ],
   "source": [
    "#Kernel interpolation with barriers\n",
    "\n",
    "arcpy.ga.KernelInterpolationWithBarriers(\n",
    "    in_features=\"NDAWN_Points\",\n",
    "    z_field=\"Avg_Temp\",\n",
    "    out_ga_layer=None,\n",
    "    out_raster=r\"C:\\ArcGIS\\Projects\\Lab3-2\\kernelNDAWN\",\n",
    "    cell_size=0.021010908,\n",
    "    in_barrier_features=None,\n",
    "    kernel_function=\"POLYNOMIAL5\",\n",
    "    bandwidth=None,\n",
    "    power=1,\n",
    "    ridge=50,\n",
    "    output_type=\"PREDICTION\"\n",
    ")\n",
    "\n",
    "print(f\"Created kernel interpolation with barriers and saved to {outputPath}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next is IDW, which I have done a lot of and see as the simplest recipe to make a spatial interpolation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created inverse distance weighted interpolation and saved to C:/ArcGIS/Projects/Lab3-2.\n"
     ]
    }
   ],
   "source": [
    "#Interpolation with IDW\n",
    "\n",
    "arcpy.ddd.Idw(\n",
    "    in_point_features=\"NDAWN_Points\",\n",
    "    z_field=\"Avg_Temp\",\n",
    "    out_raster=r\"C:\\ArcGIS\\Projects\\Lab3-2\\idwNDAWN\",\n",
    "    power=2,\n",
    "    search_radius=\"VARIABLE 12\",\n",
    "    in_barrier_polyline_features=None\n",
    ")\n",
    "\n",
    "print(f\"Created inverse distance weighted interpolation and saved to {outputPath}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for kriging, which is a stochastic interpolation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Invalid pointer ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "In  \u001b[0;34m[18]\u001b[0m:\nLine \u001b[0;34m12\u001b[0m:    ordinaryNDAWN.save(\u001b[33mr\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mC:\u001b[39;49;00m\u001b[33m\\\u001b[39;49;00m\u001b[33mArcGIS\u001b[39;49;00m\u001b[33m\\\u001b[39;49;00m\u001b[33mProjects\u001b[39;49;00m\u001b[33m\\\u001b[39;49;00m\u001b[33mLab3-2\u001b[39;49;00m\u001b[33m\\\u001b[39;49;00m\u001b[33mordinaryNDAWN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Invalid pointer \n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "#Interpolation with ordinary Kriging\n",
    "#I wrote that is \"kringing\" at first and I feel like I cannot be the only one to ever do that\n",
    "\n",
    "with arcpy.EnvManager(scratchWorkspace=outputPath):\n",
    "    ordinaryNDAWN = arcpy.sa.Kriging(\n",
    "        in_point_features=\"NDAWN_Points\",\n",
    "        z_field=\"Avg_Temp\",\n",
    "        kriging_model=\"Spherical # # # #\", #This populated automatically\n",
    "        search_radius=\"VARIABLE 12\",\n",
    "        out_variance_prediction_raster=None\n",
    "    )\n",
    "    ordinaryNDAWN.save(r\"C:\\ArcGIS\\Projects\\Lab3-2\\ordinaryNDAWN\")\n",
    "    \n",
    "print(f\"Created ordinary spherical kriging interpolation and saved to {outputPath}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above produces an error message every time I re-run it, but it DOES work the first time around.\n",
    "\n",
    "Next, I wanted to see if there was a difference between the two main kinds of kriging offered by ArcGIS Pro, ordinary (above) and universal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created universal linear drift kriging interpolation and saved to C:/ArcGIS/Projects/Lab3-2.\n"
     ]
    }
   ],
   "source": [
    "#Interpolation with universal Kriging\n",
    "\n",
    "with arcpy.EnvManager(scratchWorkspace=outputPath):\n",
    "    univNDAWN = arcpy.sa.Kriging(\n",
    "        in_point_features=\"NDAWN_Points\",\n",
    "        z_field=\"Avg_Temp\",\n",
    "        kriging_model=\"LinearDrift 0.021011 # # #\", #This populated automatically\n",
    "        search_radius=\"VARIABLE 12\",\n",
    "        out_variance_prediction_raster=None\n",
    "    )\n",
    "    univNDAWN.save(r\"C:\\ArcGIS\\Projects\\Lab3-2\\univNDAWN\")\n",
    "\n",
    "print(f\"Created universal linear drift kriging interpolation and saved to {outputPath}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### That's all, folks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArcGISPro",
   "language": "Python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
